{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Dropout, Input, LSTM, Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (75, 60000, 1)\n",
      "y_train shape: (75,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-05 23:19:07.752221: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n",
      "2024-10-05 23:19:07.752252: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-10-05 23:19:07.752262: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-10-05 23:19:07.752282: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-10-05 23:19:07.752297: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Load seismic data from the CSV file\n",
    "def load_seismic_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data\n",
    "\n",
    "# Preprocess the velocity data\n",
    "def preprocess_data(velocity_data):\n",
    "    # 1. Normalize the velocity data (zero mean, unit variance)\n",
    "    normalized_data = (velocity_data - np.mean(velocity_data)) / np.std(velocity_data)\n",
    "    \n",
    "    # 2. Reshape to add the feature dimension (required by Conv1D)\n",
    "    reshaped_data = np.expand_dims(normalized_data, axis=-1)  # Shape: (timesteps, 1 feature)\n",
    "    \n",
    "    return reshaped_data\n",
    "\n",
    "# Model architecture: 1D Convolutional Neural Network\n",
    "def build_complex_model(input_shape):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(Input(shape=input_shape))\n",
    "    \n",
    "    # First Conv1D layer\n",
    "    model.add(Conv1D(64, kernel_size=5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    # Second Conv1D layer\n",
    "    model.add(Conv1D(128, kernel_size=5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    # Optionally, you could try using LSTM for sequence learning\n",
    "    model.add(LSTM(128, return_sequences=False))\n",
    "\n",
    "    # Dense and output layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))  # Output the predicted (normalized) index\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load and process the catalog\n",
    "cat_directory = './data/lunar/training/catalogs/'\n",
    "cat_file = cat_directory + 'apollo12_catalog_GradeA_final.csv'\n",
    "cat = pd.read_csv(cat_file)\n",
    "\n",
    "# Prepare training data (seismic data + labels for quake start detection)\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# Define a max length for padding/truncating\n",
    "MAX_TIMESTEPS = 60000  # You can adjust this based on your data\n",
    "\n",
    "# Loop through the catalog\n",
    "for i, (file_name, _, start_time, _, quake_type) in cat.iterrows():\n",
    "    file_path = f\"./data/lunar/training/data/S12_GradeA/{file_name}.csv\"\n",
    "    if not os.path.isfile(file_path):\n",
    "        continue\n",
    "\n",
    "    # Load seismic data\n",
    "    data_chunk = load_seismic_data(file_path)\n",
    "    velocity = data_chunk['velocity(m/s)'].values\n",
    "    time = data_chunk['time_rel(sec)'].values\n",
    "\n",
    "    # Preprocess velocity data\n",
    "    processed_velocity = preprocess_data(velocity)  # Shape: (timesteps, 1 feature)\n",
    "\n",
    "    # Labeling: Create a label with the exact time index of the quake start\n",
    "    start_index = np.argmin(np.abs(time - start_time))  # Closest index to the start time\n",
    "    \n",
    "    # Append processed data and the start index to the training set\n",
    "    X_train.append(processed_velocity)\n",
    "    y_train.append(start_index)  # The label is the index of the quake start\n",
    "\n",
    "# Convert lists to numpy arrays with consistent time series length using padding\n",
    "X_train_padded = pad_sequences(X_train, maxlen=MAX_TIMESTEPS, dtype='float32', padding='post', truncating='post')\n",
    "\n",
    "# Convert lists to numpy arrays for model training\n",
    "X_train = np.array(X_train_padded)\n",
    "y_train = np.array(y_train)  # The labels are now the start indices\n",
    "\n",
    "# Check the shape of X_train and y_train\n",
    "print(\"X_train shape:\", X_train.shape)  # Should be (samples, timesteps, 1)\n",
    "print(\"y_train shape:\", y_train.shape)  # Should be (samples,)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "#normalize to 0-1\n",
    "y_train = y_train / MAX_TIMESTEPS  # MAX_TIMESTEPS is 60000 or the max length of the sequence\n",
    "y_val = y_val / MAX_TIMESTEPS\n",
    "\n",
    "# Build the model\n",
    "# Build the updated model\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model = build_complex_model(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-05 23:19:09.651992: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5s/step - loss: 29.7001 - mae: 4.6256 - val_loss: 23.8660 - val_mae: 4.2098\n",
      "Epoch 2/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3s/step - loss: 31.0195 - mae: 4.7731 - val_loss: 21.5260 - val_mae: 3.9409\n",
      "Epoch 3/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3s/step - loss: 24.5215 - mae: 4.1095 - val_loss: 17.3962 - val_mae: 3.5267\n",
      "Epoch 4/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3s/step - loss: 20.3688 - mae: 3.7269 - val_loss: 11.8777 - val_mae: 2.8626\n",
      "Epoch 5/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4s/step - loss: 14.9464 - mae: 3.2293 - val_loss: 7.4398 - val_mae: 2.2511\n",
      "Epoch 6/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4s/step - loss: 10.7810 - mae: 2.8680 - val_loss: 7.7991 - val_mae: 2.3013\n",
      "Epoch 7/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3s/step - loss: 8.6311 - mae: 2.5374 - val_loss: 7.7897 - val_mae: 2.2912\n",
      "Epoch 8/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4s/step - loss: 9.4530 - mae: 2.5280 - val_loss: 6.5297 - val_mae: 2.1400\n",
      "Epoch 9/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3s/step - loss: 9.6789 - mae: 2.6540 - val_loss: 5.6746 - val_mae: 2.0064\n",
      "Epoch 10/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4s/step - loss: 9.1157 - mae: 2.6131 - val_loss: 5.6016 - val_mae: 2.0047\n",
      "Epoch 11/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4s/step - loss: 8.9704 - mae: 2.5898 - val_loss: 5.6985 - val_mae: 1.9981\n",
      "Epoch 12/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3s/step - loss: 10.1025 - mae: 2.7946 - val_loss: 5.6340 - val_mae: 1.9828\n",
      "Epoch 13/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3s/step - loss: 8.8068 - mae: 2.5334 - val_loss: 5.3681 - val_mae: 1.9551\n",
      "Epoch 14/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - loss: 8.6944 - mae: 2.6685 - val_loss: 5.1676 - val_mae: 1.9216\n",
      "Epoch 15/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - loss: 8.6829 - mae: 2.5988 - val_loss: 5.0859 - val_mae: 1.8936\n",
      "Epoch 16/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - loss: 8.8357 - mae: 2.6165 - val_loss: 5.1076 - val_mae: 1.8912\n",
      "Epoch 17/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3s/step - loss: 9.6225 - mae: 2.6648 - val_loss: 5.1367 - val_mae: 1.8958\n",
      "Epoch 18/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3s/step - loss: 9.5205 - mae: 2.6986 - val_loss: 5.1348 - val_mae: 1.8957\n",
      "Epoch 19/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3s/step - loss: 10.1324 - mae: 2.7492 - val_loss: 5.0079 - val_mae: 1.8762\n",
      "Epoch 20/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3s/step - loss: 7.9520 - mae: 2.4246 - val_loss: 4.9355 - val_mae: 1.8683\n",
      "Epoch 21/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - loss: 8.9464 - mae: 2.5694 - val_loss: 4.9013 - val_mae: 1.8633\n",
      "Epoch 22/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - loss: 8.9839 - mae: 2.6190 - val_loss: 4.9181 - val_mae: 1.8649\n",
      "Epoch 23/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3s/step - loss: 7.6771 - mae: 2.4246 - val_loss: 4.9360 - val_mae: 1.8677\n",
      "Epoch 24/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4s/step - loss: 8.8981 - mae: 2.6347 - val_loss: 4.9130 - val_mae: 1.8627\n",
      "Epoch 25/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3s/step - loss: 8.9396 - mae: 2.5762 - val_loss: 4.8225 - val_mae: 1.8493\n",
      "Epoch 26/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4s/step - loss: 7.9953 - mae: 2.4611 - val_loss: 4.7601 - val_mae: 1.8325\n",
      "Epoch 27/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3s/step - loss: 8.9061 - mae: 2.6615 - val_loss: 4.6555 - val_mae: 1.8093\n",
      "Epoch 28/30\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 647ms/step - loss: 4.8298 - mae: 1.8516\n",
      "Validation MAE: 1.8516\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 78\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[39mreturn\u001b[39;00m predictions\n\u001b[1;32m     77\u001b[0m \u001b[39m# Example usage\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m predictions \u001b[39m=\u001b[39m predict_and_plot_for_all_files(model, test_directory)\n\u001b[1;32m     80\u001b[0m \u001b[39m# Output the predictions\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39mfor\u001b[39;00m file_name, predicted_start_time \u001b[39min\u001b[39;00m predictions:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Function to load seismic data\n",
    "def load_seismic_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data\n",
    "\n",
    "# Function to preprocess the velocity data\n",
    "def preprocess_data(velocity_data):\n",
    "    # Normalize the velocity data (zero mean, unit variance)\n",
    "    normalized_data = (velocity_data - np.mean(velocity_data)) / np.std(velocity_data)\n",
    "    \n",
    "    # Reshape to add the feature dimension (required by Conv1D)\n",
    "    reshaped_data = np.expand_dims(normalized_data, axis=-1)  # Shape: (timesteps, 1 feature)\n",
    "    \n",
    "    return reshaped_data\n",
    "\n",
    "# Define the directory for test files\n",
    "test_directory = \"./data/lunar/test/data/S12_GradeB/\"\n",
    "output_plot_dir = \"./predictions_plots_v1/\"  # Directory to save plots\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_plot_dir, exist_ok=True)\n",
    "\n",
    "# Function to predict moonquake start and save plot for each test file\n",
    "def predict_and_plot_for_all_files(model, test_directory, max_timesteps=60000):\n",
    "    predictions = []  # Store the results here\n",
    "    \n",
    "    # Iterate through all the CSV files in the test directory\n",
    "    for file_name in os.listdir(test_directory):\n",
    "        if file_name.endswith(\".csv\"):  # Only process CSV files\n",
    "            file_path = os.path.join(test_directory, file_name)\n",
    "            print(f\"Processing file: {file_name}\")  # For debugging/logging\n",
    "            \n",
    "            # Load seismic data from the file\n",
    "            data_chunk = load_seismic_data(file_path)\n",
    "            velocity = data_chunk['velocity(m/s)'].values\n",
    "            time = data_chunk['time_rel(sec)'].values\n",
    "            \n",
    "            # Preprocess the velocity data\n",
    "            processed_velocity = preprocess_data(velocity)\n",
    "            processed_velocity = pad_sequences([processed_velocity], maxlen=max_timesteps, dtype='float32', padding='post', truncating='post')\n",
    "\n",
    "            # Make prediction\n",
    "            predicted_index = model.predict(processed_velocity)\n",
    "            predicted_index = int(predicted_index[0] * MAX_TIMESTEPS)  # Scale back to original range\n",
    "            \n",
    "            # Get the predicted start time using the index\n",
    "            predicted_start_time = time[predicted_index]\n",
    "\n",
    "            # Plotting the seismic data and the predicted moonquake start\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(time, velocity, label=\"Seismic Velocity Data\", color='blue')\n",
    "            \n",
    "            # Add a red line for the predicted quake start time\n",
    "            plt.axvline(x=predicted_start_time, color='red', linestyle='--', label=f\"Predicted Quake Start: {predicted_start_time:.2f}s\")\n",
    "            \n",
    "            plt.title(f\"Moonquake Prediction for {file_name}\")\n",
    "            plt.xlabel(\"Time (seconds)\")\n",
    "            plt.ylabel(\"Velocity (m/s)\")\n",
    "            plt.legend()\n",
    "            \n",
    "            # Save the plot to a file\n",
    "            plot_filename = os.path.join(output_plot_dir, f\"{file_name}_prediction_plot.png\")\n",
    "            plt.savefig(plot_filename)\n",
    "            plt.close()  # Close the plot to avoid displaying it during processing\n",
    "\n",
    "            # Store the result (file_name, predicted_start_time)\n",
    "            predictions.append((file_name, predicted_start_time))\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Example usage\n",
    "predictions = predict_and_plot_for_all_files(model, test_directory)\n",
    "\n",
    "# Output the predictions\n",
    "for file_name, predicted_start_time in predictions:\n",
    "    print(f\"File: {file_name}, Predicted Moonquake Start Time: {predicted_start_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismic_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
