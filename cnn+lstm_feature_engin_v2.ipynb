{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seismic detection using neural network with CNN and LSTM layers combined"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Dropout, Input, LSTM, Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load seismic data from the CSV file\n",
    "def load_seismic_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data\n",
    "\n",
    "# Preprocess the velocity data\n",
    "def preprocess_data(velocity_data):\n",
    "    # Normalize the velocity data (zero mean, unit variance)\n",
    "    normalized_data = (velocity_data - np.mean(velocity_data)) / np.std(velocity_data)\n",
    "    \n",
    "    return normalized_data\n",
    "\n",
    "def calculate_derivative(velocity_data):\n",
    "    # Get the derivative feature at each time step\n",
    "    derivative = np.diff(velocity_data, prepend=velocity_data[0])  # Calculate the derivative\n",
    "    return derivative\n",
    "\n",
    "def preprocess_derivative(derivative_data):\n",
    "    # Normalize the derivative values\n",
    "    normalized_derivative = (derivative_data - np.mean(derivative_data)) / np.std(derivative_data)\n",
    "    return normalized_derivative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture: 1D Convolutional Neural Network\n",
    "def build_model_with_lstm(input_shape):\n",
    "    model = Sequential()\n",
    "\n",
    "   # Input layer\n",
    "    model.add(Input(shape=input_shape))\n",
    "    \n",
    "    # First Conv1D layer\n",
    "    model.add(Conv1D(64, kernel_size=5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    # Second Conv1D layer\n",
    "    model.add(Conv1D(128, kernel_size=5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    # Using LSTM for sequence learning\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=False)))\n",
    "\n",
    "    # Dense and output layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5)) # Dropout to prevent overfitting\n",
    "    model.add(Dense(1))  # Output the predicted (normalized) index\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (75, 60000, 2)\n",
      "y_train shape: (75,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-06 10:34:37.569732: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n",
      "2024-10-06 10:34:37.569782: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-10-06 10:34:37.569789: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-10-06 10:34:37.569819: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-10-06 10:34:37.569841: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Load and process the catalog\n",
    "cat_directory = './data/lunar/training/catalogs/'\n",
    "cat_file = cat_directory + 'apollo12_catalog_GradeA_final.csv'\n",
    "cat = pd.read_csv(cat_file)\n",
    "\n",
    "# Prepare training data (seismic data + labels for quake start detection)\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# Define a max length for padding/truncating\n",
    "MAX_TIMESTEPS = 60000  # the maximum expected recording length\n",
    "\n",
    "# Loop through the catalog\n",
    "for i, (file_name, _, start_time, _, quake_type) in cat.iterrows():\n",
    "    file_path = f\"./data/lunar/training/data/S12_GradeA/{file_name}.csv\"\n",
    "    if not os.path.isfile(file_path):\n",
    "        continue\n",
    "\n",
    "    # Load seismic data\n",
    "    data_chunk = load_seismic_data(file_path)\n",
    "    velocity = data_chunk['velocity(m/s)'].values\n",
    "    time = data_chunk['time_rel(sec)'].values\n",
    "\n",
    "    # Preprocess the data\n",
    "    velocity_derivative = calculate_derivative(velocity)\n",
    "    processed_velocity = preprocess_data(velocity)\n",
    "    processed_derivative = preprocess_derivative(velocity_derivative)\n",
    "\n",
    "    # Stack both velocity and its derivative as input features\n",
    "    stacked_data = np.stack((processed_velocity, processed_derivative), axis=-1)  # Shape: (timesteps, 2 features)\n",
    "\n",
    "    # Labeling: Create a label with the exact time index of the quake start\n",
    "    start_index = np.argmin(np.abs(time - start_time))  # Closest index to the start time\n",
    "    \n",
    "    # Append processed data and the start index to the training set\n",
    "    X_train.append(stacked_data)\n",
    "    y_train.append(start_index)  # The label is the index of the quake start\n",
    "\n",
    "# Convert lists to numpy arrays with consistent time series length using padding\n",
    "X_train_padded = pad_sequences(X_train, maxlen=MAX_TIMESTEPS, dtype='float32', padding='post', truncating='post')\n",
    "\n",
    "# Convert lists to numpy arrays for model training\n",
    "X_train = np.array(X_train_padded)\n",
    "y_train = np.array(y_train)  # The labels are now the start indices\n",
    "\n",
    "# Check the shape of X_train and y_train\n",
    "print(\"X_train shape:\", X_train.shape)  # Should be (samples, timesteps, 1)\n",
    "print(\"y_train shape:\", y_train.shape)  # Should be (samples,)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "#normalize to 0-1\n",
    "y_train = y_train / MAX_TIMESTEPS  # MAX_TIMESTEPS is 60000 or the max length of the sequence\n",
    "y_val = y_val / MAX_TIMESTEPS\n",
    "\n",
    "# Build the model\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model = build_model_with_lstm(input_shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2/2 - 7s - 3s/step - loss: 8.1913 - mae: 2.5096 - val_loss: 4.7614 - val_mae: 1.8573\n",
      "Epoch 2/30\n",
      "2/2 - 6s - 3s/step - loss: 8.3567 - mae: 2.5087 - val_loss: 4.7222 - val_mae: 1.8626\n",
      "Epoch 3/30\n",
      "2/2 - 6s - 3s/step - loss: 8.0280 - mae: 2.5400 - val_loss: 4.6695 - val_mae: 1.8693\n",
      "Epoch 4/30\n",
      "2/2 - 6s - 3s/step - loss: 8.9551 - mae: 2.6352 - val_loss: 4.6282 - val_mae: 1.8675\n",
      "Epoch 5/30\n",
      "2/2 - 6s - 3s/step - loss: 7.7815 - mae: 2.4365 - val_loss: 4.5892 - val_mae: 1.8522\n",
      "Epoch 6/30\n",
      "2/2 - 7s - 3s/step - loss: 7.2905 - mae: 2.3453 - val_loss: 4.5430 - val_mae: 1.8357\n",
      "Epoch 7/30\n",
      "2/2 - 6s - 3s/step - loss: 7.9147 - mae: 2.5103 - val_loss: 4.5122 - val_mae: 1.8389\n",
      "Epoch 8/30\n",
      "2/2 - 6s - 3s/step - loss: 7.1787 - mae: 2.3237 - val_loss: 4.5237 - val_mae: 1.8716\n",
      "Epoch 9/30\n",
      "2/2 - 6s - 3s/step - loss: 7.6946 - mae: 2.3877 - val_loss: 4.6423 - val_mae: 1.9114\n",
      "Epoch 10/30\n",
      "2/2 - 6s - 3s/step - loss: 7.0228 - mae: 2.2793 - val_loss: 4.7810 - val_mae: 1.9376\n",
      "Epoch 11/30\n",
      "2/2 - 6s - 3s/step - loss: 7.8049 - mae: 2.4258 - val_loss: 4.7036 - val_mae: 1.9290\n",
      "Epoch 12/30\n",
      "2/2 - 6s - 3s/step - loss: 7.2922 - mae: 2.3175 - val_loss: 4.4497 - val_mae: 1.8661\n",
      "Epoch 13/30\n",
      "2/2 - 5s - 3s/step - loss: 7.2575 - mae: 2.3579 - val_loss: 4.5066 - val_mae: 1.8677\n",
      "Epoch 14/30\n",
      "2/2 - 6s - 3s/step - loss: 7.0526 - mae: 2.2618 - val_loss: 4.6312 - val_mae: 1.9110\n",
      "Epoch 15/30\n",
      "2/2 - 6s - 3s/step - loss: 6.6588 - mae: 2.2266 - val_loss: 4.7088 - val_mae: 1.9251\n",
      "Epoch 16/30\n",
      "2/2 - 6s - 3s/step - loss: 6.3269 - mae: 2.1488 - val_loss: 4.6828 - val_mae: 1.9136\n",
      "Epoch 17/30\n",
      "2/2 - 6s - 3s/step - loss: 6.9893 - mae: 2.2730 - val_loss: 4.4993 - val_mae: 1.8697\n",
      "Epoch 18/30\n",
      "2/2 - 6s - 3s/step - loss: 6.9150 - mae: 2.2912 - val_loss: 4.5028 - val_mae: 1.8592\n",
      "Epoch 19/30\n",
      "2/2 - 6s - 3s/step - loss: 5.8245 - mae: 2.0432 - val_loss: 4.5766 - val_mae: 1.8715\n",
      "Epoch 20/30\n",
      "2/2 - 6s - 3s/step - loss: 6.1949 - mae: 2.0940 - val_loss: 4.5827 - val_mae: 1.8557\n",
      "Epoch 21/30\n",
      "2/2 - 6s - 3s/step - loss: 5.6490 - mae: 2.0085 - val_loss: 4.5572 - val_mae: 1.8535\n",
      "Epoch 22/30\n",
      "2/2 - 6s - 3s/step - loss: 5.8030 - mae: 2.0045 - val_loss: 4.6494 - val_mae: 1.8649\n",
      "Epoch 23/30\n",
      "2/2 - 6s - 3s/step - loss: 6.3082 - mae: 2.0758 - val_loss: 4.6587 - val_mae: 1.8533\n",
      "Epoch 24/30\n",
      "2/2 - 6s - 3s/step - loss: 5.6573 - mae: 2.0359 - val_loss: 4.4491 - val_mae: 1.8009\n",
      "Epoch 25/30\n",
      "2/2 - 6s - 3s/step - loss: 5.9936 - mae: 2.0906 - val_loss: 4.4906 - val_mae: 1.8028\n",
      "Epoch 26/30\n",
      "2/2 - 6s - 3s/step - loss: 5.3687 - mae: 2.0186 - val_loss: 4.8858 - val_mae: 1.8378\n",
      "Epoch 27/30\n",
      "2/2 - 6s - 3s/step - loss: 5.2072 - mae: 1.8845 - val_loss: 5.1670 - val_mae: 1.8752\n",
      "Epoch 28/30\n",
      "2/2 - 6s - 3s/step - loss: 6.2130 - mae: 2.0406 - val_loss: 5.2484 - val_mae: 1.9029\n",
      "Epoch 29/30\n",
      "2/2 - 6s - 3s/step - loss: 5.4830 - mae: 1.9461 - val_loss: 5.2161 - val_mae: 1.9397\n",
      "Epoch 30/30\n",
      "2/2 - 6s - 3s/step - loss: 5.1658 - mae: 1.8773 - val_loss: 4.6932 - val_mae: 1.8228\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_val, y_val),verbose=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 4.6932 - mae: 1.8228\n",
      "Validation MAE: 1.8228\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation MAE: {mae:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code outputs prediction to both CSV files (that can be used later to score the model), and images, which provides the ability to visually assess the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: xa.s12.00.mhz.1970-05-23HR00_evid00027.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "Processing file: xa.s12.00.mhz.1977-04-11HR00_evid00915.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n",
      "Processing file: xa.s12.00.mhz.1970-07-18HR00_evid00036.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n",
      "Processing file: xa.s12.00.mhz.1971-11-24HR00_evid00156.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n",
      "Processing file: xa.s12.00.mhz.1972-12-06HR00_evid00342.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n",
      "Processing file: xa.s12.00.mhz.1974-03-14HR00_evid00506.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n",
      "Processing file: xa.s12.00.mhz.1973-11-22HR00_evid00475.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n",
      "Processing file: xa.s12.00.mhz.1971-04-08HR01_evid00083.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step\n",
      "Processing file: xa.s12.00.mhz.1973-10-10HR00_evid00463.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n",
      "Processing file: xa.s12.00.mhz.1974-07-25HR05_evid00553.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n",
      "Processing file: xa.s12.00.mhz.1972-02-21HR00_evid00190.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step\n",
      "Processing file: xa.s12.00.mhz.1974-05-09HR00_evid00522.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step\n",
      "Processing file: xa.s12.00.mhz.1970-10-26HR00_evid00049.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step\n",
      "Processing file: xa.s12.00.mhz.1972-12-02HR00_evid00341.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n",
      "Processing file: xa.s12.00.mhz.1972-11-14HR00_evid00331.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step\n",
      "Processing file: xa.s12.00.mhz.1972-12-15HR00_evid00349.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step\n",
      "Processing file: xa.s12.00.mhz.1970-06-19HR00_evid00031.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n",
      "Processing file: xa.s12.00.mhz.1971-06-11HR00_evid00096.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step\n",
      "Processing file: xa.s12.00.mhz.1970-07-27HR00_evid00039.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n",
      "Processing file: xa.s12.00.mhz.1970-03-30HR00_evid00020.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n",
      "Processing file: xa.s12.00.mhz.1970-11-03HR00_evid00051.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n",
      "Processing file: xa.s12.00.mhz.1974-06-30HR00_evid00543.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n",
      "Processing file: xa.s12.00.mhz.1970-11-03HR00_evid00050.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n",
      "Processing file: xa.s12.00.mhz.1971-01-17HR00_evid00060.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step\n",
      "Processing file: xa.s12.00.mhz.1971-01-03HR00_evid00057.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step\n",
      "Processing file: xa.s12.00.mhz.1969-12-16HR00_evid00006.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n",
      "Processing file: xa.s12.00.mhz.1971-10-26HR00_evid00133.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step\n",
      "Processing file: xa.s12.00.mhz.1970-05-20HR00_evid00026.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step\n",
      "Processing file: xa.s12.00.mhz.1975-04-21HR00_evid00638.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step\n",
      "Processing file: xa.s12.00.mhz.1973-03-12HR00_evid00384.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n",
      "Processing file: xa.s12.00.mhz.1970-07-17HR00_evid00035.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n",
      "Processing file: xa.s12.00.mhz.1973-10-03HR03_evid00461.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n",
      "Processing file: xa.s12.00.mhz.1970-03-14HR00_evid00018.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n",
      "Processing file: xa.s12.00.mhz.1975-05-20HR00_evid00652.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n",
      "Processing file: xa.s12.00.mhz.1977-07-19HR00_evid00991.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n",
      "Processing file: xa.s12.00.mhz.1972-01-26HR00_evid00186.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step\n",
      "Processing file: xa.s12.00.mhz.1971-03-15HR00_evid00073.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n",
      "Processing file: xa.s12.00.mhz.1977-04-26HR00_evid00924.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n",
      "Processing file: xa.s12.00.mhz.1970-05-25HR00_evid00029.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n",
      "Processing file: xa.s12.00.mhz.1972-05-19HR00_evid00228.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n",
      "Processing file: xa.s12.00.mhz.1973-08-08HR00_evid00437.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step\n",
      "Processing file: xa.s12.00.mhz.1970-04-03HR00_evid00021.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n",
      "Processing file: xa.s12.00.mhz.1970-02-07HR00_evid00014.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n",
      "Processing file: xa.s12.00.mhz.1971-10-06HR00_evid00124.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n",
      "Processing file: xa.s12.00.mhz.1972-05-15HR00_evid00223.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step\n",
      "Processing file: xa.s12.00.mhz.1971-10-06HR00_evid00125.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n",
      "Processing file: xa.s12.00.mhz.1975-06-15HR00_evid00660.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n",
      "Processing file: xa.s12.00.mhz.1970-05-24HR00_evid00028.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n",
      "Processing file: xa.s12.00.mhz.1974-10-02HR00_evid00572.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n",
      "Processing file: xa.s12.00.mhz.1972-11-19HR00_evid00335.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n",
      "Processing file: xa.s12.00.mhz.1971-01-05HR00_evid00059.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n",
      "Processing file: xa.s12.00.mhz.1972-02-28HR00_evid00192.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\n",
      "Processing file: xa.s12.00.mhz.1970-09-09HR00_evid00043.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n",
      "Processing file: xa.s12.00.mhz.1971-06-27HR00_evid00101.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n",
      "Processing file: xa.s12.00.mhz.1971-05-22HR00_evid00092.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n",
      "Processing file: xa.s12.00.mhz.1975-05-16HR00_evid00651.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step\n",
      "Processing file: xa.s12.00.mhz.1970-02-18HR00_evid00016.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step\n",
      "Processing file: xa.s12.00.mhz.1970-01-09HR00_evid00007.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step\n",
      "Processing file: xa.s12.00.mhz.1977-09-13HR00_evid01012.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n",
      "Processing file: xa.s12.00.mhz.1975-06-17HR00_evid00662.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n",
      "Processing file: xa.s12.00.mhz.1970-07-20HR00_evid00037.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step\n",
      "Processing file: xa.s12.00.mhz.1977-04-24HR00_evid00923.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step\n",
      "Processing file: xa.s12.00.mhz.1974-03-30HR00_evid00512.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n",
      "Processing file: xa.s12.00.mhz.1973-04-23HR00_evid00399.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n",
      "File: xa.s12.00.mhz.1970-05-23HR00_evid00027.csv, Predicted Moonquake Start Time: 8177.056603773585\n",
      "File: xa.s12.00.mhz.1977-04-11HR00_evid00915.csv, Predicted Moonquake Start Time: 43380.981132075474\n",
      "File: xa.s12.00.mhz.1970-07-18HR00_evid00036.csv, Predicted Moonquake Start Time: 44653.73584905661\n",
      "File: xa.s12.00.mhz.1971-11-24HR00_evid00156.csv, Predicted Moonquake Start Time: 50031.3962264151\n",
      "File: xa.s12.00.mhz.1972-12-06HR00_evid00342.csv, Predicted Moonquake Start Time: 60137.50943396227\n",
      "File: xa.s12.00.mhz.1974-03-14HR00_evid00506.csv, Predicted Moonquake Start Time: 42703.54716981132\n",
      "File: xa.s12.00.mhz.1973-11-22HR00_evid00475.csv, Predicted Moonquake Start Time: 44193.811320754714\n",
      "File: xa.s12.00.mhz.1971-04-08HR01_evid00083.csv, Predicted Moonquake Start Time: 29339.622641509435\n",
      "File: xa.s12.00.mhz.1973-10-10HR00_evid00463.csv, Predicted Moonquake Start Time: 71397.1320754717\n",
      "File: xa.s12.00.mhz.1974-07-25HR05_evid00553.csv, Predicted Moonquake Start Time: 23532.075471698114\n",
      "File: xa.s12.00.mhz.1972-02-21HR00_evid00190.csv, Predicted Moonquake Start Time: 44103.84905660377\n",
      "File: xa.s12.00.mhz.1974-05-09HR00_evid00522.csv, Predicted Moonquake Start Time: 59648.45283018868\n",
      "File: xa.s12.00.mhz.1970-10-26HR00_evid00049.csv, Predicted Moonquake Start Time: 55439.09433962264\n",
      "File: xa.s12.00.mhz.1972-12-02HR00_evid00341.csv, Predicted Moonquake Start Time: 43402.26415094339\n",
      "File: xa.s12.00.mhz.1972-11-14HR00_evid00331.csv, Predicted Moonquake Start Time: 24896.150943396227\n",
      "File: xa.s12.00.mhz.1972-12-15HR00_evid00349.csv, Predicted Moonquake Start Time: 49432.301886792455\n",
      "File: xa.s12.00.mhz.1970-06-19HR00_evid00031.csv, Predicted Moonquake Start Time: 40795.92452830189\n",
      "File: xa.s12.00.mhz.1971-06-11HR00_evid00096.csv, Predicted Moonquake Start Time: 44996.981132075474\n",
      "File: xa.s12.00.mhz.1970-07-27HR00_evid00039.csv, Predicted Moonquake Start Time: 45353.056603773584\n",
      "File: xa.s12.00.mhz.1970-03-30HR00_evid00020.csv, Predicted Moonquake Start Time: 15691.32075471698\n",
      "File: xa.s12.00.mhz.1970-11-03HR00_evid00051.csv, Predicted Moonquake Start Time: 39000.6037735849\n",
      "File: xa.s12.00.mhz.1974-06-30HR00_evid00543.csv, Predicted Moonquake Start Time: 49703.3962264151\n",
      "File: xa.s12.00.mhz.1970-11-03HR00_evid00050.csv, Predicted Moonquake Start Time: 39000.6037735849\n",
      "File: xa.s12.00.mhz.1971-01-17HR00_evid00060.csv, Predicted Moonquake Start Time: 49399.3962264151\n",
      "File: xa.s12.00.mhz.1971-01-03HR00_evid00057.csv, Predicted Moonquake Start Time: 30838.64150943396\n",
      "File: xa.s12.00.mhz.1969-12-16HR00_evid00006.csv, Predicted Moonquake Start Time: 44116.22641509434\n",
      "File: xa.s12.00.mhz.1971-10-26HR00_evid00133.csv, Predicted Moonquake Start Time: 48991.24528301887\n",
      "File: xa.s12.00.mhz.1970-05-20HR00_evid00026.csv, Predicted Moonquake Start Time: 44109.1320754717\n",
      "File: xa.s12.00.mhz.1975-04-21HR00_evid00638.csv, Predicted Moonquake Start Time: 67180.22641509434\n",
      "File: xa.s12.00.mhz.1973-03-12HR00_evid00384.csv, Predicted Moonquake Start Time: 17692.075471698114\n",
      "File: xa.s12.00.mhz.1970-07-17HR00_evid00035.csv, Predicted Moonquake Start Time: 32348.67924528302\n",
      "File: xa.s12.00.mhz.1973-10-03HR03_evid00461.csv, Predicted Moonquake Start Time: 57320.0\n",
      "File: xa.s12.00.mhz.1970-03-14HR00_evid00018.csv, Predicted Moonquake Start Time: 20799.245283018867\n",
      "File: xa.s12.00.mhz.1975-05-20HR00_evid00652.csv, Predicted Moonquake Start Time: 38931.16981132075\n",
      "File: xa.s12.00.mhz.1977-07-19HR00_evid00991.csv, Predicted Moonquake Start Time: 28476.075471698114\n",
      "File: xa.s12.00.mhz.1972-01-26HR00_evid00186.csv, Predicted Moonquake Start Time: 31435.169811320753\n",
      "File: xa.s12.00.mhz.1971-03-15HR00_evid00073.csv, Predicted Moonquake Start Time: 67252.83018867925\n",
      "File: xa.s12.00.mhz.1977-04-26HR00_evid00924.csv, Predicted Moonquake Start Time: 24596.98113207547\n",
      "File: xa.s12.00.mhz.1970-05-25HR00_evid00029.csv, Predicted Moonquake Start Time: 68902.03773584905\n",
      "File: xa.s12.00.mhz.1972-05-19HR00_evid00228.csv, Predicted Moonquake Start Time: 12706.566037735847\n",
      "File: xa.s12.00.mhz.1973-08-08HR00_evid00437.csv, Predicted Moonquake Start Time: 61563.47169811321\n",
      "File: xa.s12.00.mhz.1970-04-03HR00_evid00021.csv, Predicted Moonquake Start Time: 33698.264150943396\n",
      "File: xa.s12.00.mhz.1970-02-07HR00_evid00014.csv, Predicted Moonquake Start Time: 52699.16981132075\n",
      "File: xa.s12.00.mhz.1971-10-06HR00_evid00124.csv, Predicted Moonquake Start Time: 8061.132075471698\n",
      "File: xa.s12.00.mhz.1972-05-15HR00_evid00223.csv, Predicted Moonquake Start Time: 52892.52830188679\n",
      "File: xa.s12.00.mhz.1971-10-06HR00_evid00125.csv, Predicted Moonquake Start Time: 8061.132075471698\n",
      "File: xa.s12.00.mhz.1975-06-15HR00_evid00660.csv, Predicted Moonquake Start Time: 44115.47169811321\n",
      "File: xa.s12.00.mhz.1970-05-24HR00_evid00028.csv, Predicted Moonquake Start Time: 16336.150943396226\n",
      "File: xa.s12.00.mhz.1974-10-02HR00_evid00572.csv, Predicted Moonquake Start Time: 44202.56603773585\n",
      "File: xa.s12.00.mhz.1972-11-19HR00_evid00335.csv, Predicted Moonquake Start Time: 44801.35849056604\n",
      "File: xa.s12.00.mhz.1971-01-05HR00_evid00059.csv, Predicted Moonquake Start Time: 52612.83018867925\n",
      "File: xa.s12.00.mhz.1972-02-28HR00_evid00192.csv, Predicted Moonquake Start Time: 27201.35849056604\n",
      "File: xa.s12.00.mhz.1970-09-09HR00_evid00043.csv, Predicted Moonquake Start Time: 52827.018867924526\n",
      "File: xa.s12.00.mhz.1971-06-27HR00_evid00101.csv, Predicted Moonquake Start Time: 33002.11320754717\n",
      "File: xa.s12.00.mhz.1971-05-22HR00_evid00092.csv, Predicted Moonquake Start Time: 54346.41509433962\n",
      "File: xa.s12.00.mhz.1975-05-16HR00_evid00651.csv, Predicted Moonquake Start Time: 37821.88679245283\n",
      "File: xa.s12.00.mhz.1970-02-18HR00_evid00016.csv, Predicted Moonquake Start Time: 45891.16981132075\n",
      "File: xa.s12.00.mhz.1970-01-09HR00_evid00007.csv, Predicted Moonquake Start Time: 21988.830188679247\n",
      "File: xa.s12.00.mhz.1977-09-13HR00_evid01012.csv, Predicted Moonquake Start Time: 43710.64150943396\n",
      "File: xa.s12.00.mhz.1975-06-17HR00_evid00662.csv, Predicted Moonquake Start Time: 51354.26415094339\n",
      "File: xa.s12.00.mhz.1970-07-20HR00_evid00037.csv, Predicted Moonquake Start Time: 42971.622641509435\n",
      "File: xa.s12.00.mhz.1977-04-24HR00_evid00923.csv, Predicted Moonquake Start Time: 55354.71698113208\n",
      "File: xa.s12.00.mhz.1974-03-30HR00_evid00512.csv, Predicted Moonquake Start Time: 73601.35849056604\n",
      "File: xa.s12.00.mhz.1973-04-23HR00_evid00399.csv, Predicted Moonquake Start Time: 50726.79245283019\n"
     ]
    }
   ],
   "source": [
    "# Define the directory for test files\n",
    "test_directory = \"./data/lunar/test/data/S12_GradeB/\"\n",
    "output_plot_dir = \"./predictions_plots_v2/\"  # Directory to save plots\n",
    "output_csv_file = \"./predictions_output_v2.csv\"  # CSV file to save the predictions\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_plot_dir, exist_ok=True)\n",
    "\n",
    "# Function to predict moonquake start, save plot, and store results in a CSV\n",
    "def predict_and_plot_for_all_files(model, test_directory, max_timesteps=60000):\n",
    "    predictions = []  # Store the results here\n",
    "    \n",
    "    # Open CSV file for writing\n",
    "    with open(output_csv_file, mode='w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        # Write the header for the CSV file\n",
    "        csv_writer.writerow([\"filename\", \"time_rel(sec)\"]) # Write the column names\n",
    "        \n",
    "        # Iterate through all the CSV test files in the test directory\n",
    "        for file_name in os.listdir(test_directory):\n",
    "            if file_name.endswith(\".csv\"):  # Only process CSV files\n",
    "                file_path = os.path.join(test_directory, file_name)\n",
    "                print(f\"Processing file: {file_name}\")  # For debugging/logging\n",
    "                \n",
    "                # Load seismic data from the file\n",
    "                data_chunk = load_seismic_data(file_path)\n",
    "                velocity = data_chunk['velocity(m/s)'].values\n",
    "                time = data_chunk['time_rel(sec)'].values\n",
    "                \n",
    "                velocity_derivative = calculate_derivative(velocity)\n",
    "                processed_velocity = preprocess_data(velocity)\n",
    "                processed_derivative = preprocess_derivative(velocity_derivative)\n",
    "\n",
    "                # Stack both velocity and its derivative as input features\n",
    "                stacked_data = np.stack((processed_velocity, processed_derivative), axis=-1)  # Shape: (timesteps, 2 features)\n",
    "                \n",
    "                # Preprocess the velocity data\n",
    "                stacked_data = pad_sequences([stacked_data], maxlen=max_timesteps, dtype='float32', padding='post', truncating='post')\n",
    "\n",
    "                # Make prediction\n",
    "                predicted_index = model.predict(stacked_data)\n",
    "                predicted_index = int(predicted_index[0] * MAX_TIMESTEPS)  # Scale back to original range\n",
    "                \n",
    "                # Get the predicted start time using the index\n",
    "                predicted_start_time = time[predicted_index]\n",
    "\n",
    "                # Plotting the seismic data and the predicted moonquake start\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.plot(time, velocity, label=\"Seismic Velocity Data\", color='blue')\n",
    "                \n",
    "                # Add a red line for the predicted quake start time\n",
    "                plt.axvline(x=predicted_start_time, color='red', linestyle='--', label=f\"Predicted Quake Start: {predicted_start_time:.2f}s\")\n",
    "                \n",
    "                plt.title(f\"Moonquake Prediction for {file_name}\")\n",
    "                plt.xlabel(\"Time (seconds)\")\n",
    "                plt.ylabel(\"Velocity (m/s)\")\n",
    "                plt.legend()\n",
    "                \n",
    "                # Save the plot to a file\n",
    "                plot_filename = os.path.join(output_plot_dir, f\"{file_name}_prediction_plot.png\")\n",
    "                plt.savefig(plot_filename)\n",
    "                plt.close()  # Close the plot to avoid displaying it during processing\n",
    "\n",
    "                # Store the result (file_name, predicted_start_time) in the CSV\n",
    "                csv_writer.writerow([file_name, predicted_start_time])\n",
    "\n",
    "                # Append the result to the predictions list for further use if needed\n",
    "                predictions.append((file_name, predicted_start_time))\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Example usage\n",
    "predictions = predict_and_plot_for_all_files(model, test_directory)\n",
    "\n",
    "# Output the predictions\n",
    "for file_name, predicted_start_time in predictions:\n",
    "    print(f\"File: {file_name}, Predicted Moonquake Start Time: {predicted_start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismic_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
